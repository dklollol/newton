\documentclass[a4paper,12pt]{article}

\title{REX Exam 2016}
\author{Group 6:\\Niels\\Troels\\Mark\\Kristian}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{microtype}
\usepackage{underscore}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}

\setlength{\parskip}{1ex}
\setlength{\parindent}{0pt}
\setlength{\parfillskip}{30pt plus 1 fil}

\begin{document}

\maketitle
\newpage

\section{Core Implementation}

To build the code, run \texttt{make}.  It builds the program \texttt{main}.  We
use OpenCV 2.4.

Our main loop works like this:

\begin{enumerate}
\item Get a frame from the webcam.
\item Detect landmarks by calling \texttt{cam.get_object}.
\item Compute particle weights.
\item Resample particles.
\item Move the robot according to our strategy.
\item Update particles according to how much the robot has moved.
\item Add uncertainty.
\item Estimate pose.
\item Draw and render for debugging purposes.
\end{enumerate}

See \texttt{run} in \texttt{main.cc} for the implementation.

We set the Scorpion robot to always drive with 10 cm per second, and always turn
with 20 degrees per second.


\subsection{Code Profiling}

We would like our solution to be as fast as possible, so we have timed the parts
of the main loop to see what might be improved.

Most of the steps complete in less than 20 milliseconds.  Only landmark
detection and robot moving are really time-consuming steps.  We do not dare to
change the handed out landmark detection code, and we cannot reliably make the
robot move any faster, so we are stuck with a main loop that spends at least 300
milliseconds in each iteration.


\subsection{Webcam Hack}

Since each iteration of the main loop is pretty slow, and since the webcam
updates pretty often, we don't use \emph{all} webcam frames, but only the newest
one whenever the code starts a new loop iteration.

However, OpenCV uses a FIFO buffer for grabbing webcam frames, so that you get
the first frame in the buffer and not necessarily the \emph{newest} frame.  The
solution, which is in the handed out code, is to set the property
\texttt{CV_CAP_PROP_BUFFERSIZE} to $1$, but this property does not work with all
webcams\footnote{\url{http://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html}},
including ours.

We had to comment out the \texttt{CV_CAP_PROP_BUFFERSIZE} fix, which resulted in
us getting outdated frames from our webcam, meaning we had to find another fix.

Our current fix works by starting a thread which continually grabs frames from
the camera and always stores the newest frame in a global variable, which the
main loop can then access.  We use a couple of synchronization primitives to
avoid race conditions.  It appears to work, although it would have been nicer
had our webcam supported the OpenCV property.


\newpage
\section{Particle Filter}

We use the same particle filter as in our exercise 5, with a few additions:

\subsection{Adjustable Variance}

When adding noise to the particles, we vary the noise based on the actions of
the robot:

\begin{itemize}
\item If the robot has not moved, we add a small, constant amount of noise.
\item If the robot has driven forward, but not turned, we add a small angle
noise, and a larger position noise dependent on how much it has moved.
\item If the robot has turned, but not driven anywhere, we add a small position
noise, and a larger angle noise dependent on how much it has turned.
\end{itemize}

We belive this might improve the precision of our particle filter.  See the
function \texttt{command_variance} in \texttt{misc.cc} for the implementation.


\subsection{Better Landmark Weighting}

We have extended the \texttt{object} type to also have \texttt{landmarkN}
entries, where $\texttt{N} \in [1..4]$, which are then used when assigning
weights to the particles.


\subsection{Random Particles}

To make the particle filter more secure against e.g. teleportations, we add a
few random particles after assigning weights.


\newpage
\section{Driving Strategy}

When driving, the distance is split into 10 moves.  After each move the code
might choose to check the IR sensors for obstacles.  If an obstacle is found,
the robot turns and drives around it.

We use a state-based approach for our driving strategy: We split the strategy
into multiple states, and let each state handle a small part.

\begin{description}
\item[searching_random]\hfill\\
This is the starting state (on the odd chance that the robot cannot initially
see the first landmark).

In this state the robot drives and turns randomly until it detects a landmark,
after which it goes into state \textbf{approach}.


\item[searching_square]\hfill\\
This state is activated whenever the robot has arrived at a landmark.  In this
state the robot drives around the landmark while it tries to detect the next
landmark to visit.  This is necessary in at least the beginning, since we cannot
trust our particle filter until we have detected at least two landmarks (after
which a single cluster should be forming).

If a landmark is detected, it goes into state \textbf{approach}.  If no landmark
is detected after driving a full square, it goes into state
\textbf{searching_random}, after which it will hopefully go into state
\textbf{approach} at some point.

This state enables IR sensor obstacle avoidance.


\item[approach]\hfill\\
In this state the robot approaches a detected landmark, either using just the
webcam -- if we have only seen a single landmark -- or the particle filter.  It
goes to either state \textbf{align} or state \textbf{goto_landmark}.


\item[align]\hfill\\
The robot aligns itself to look directly at the current landmark.  When done, it
goes back to state \textbf{approach}.


\item[goto_landmark]\hfill\\
This state uses the particle filter to determine what robot moves are necessary
for driving to the next landmark, after which it executes these actions.  When
it has arrived at the landmark, it goes to state \textbf{searching_square} if
there are more landmarks left, or state \textbf{finished} if the visited
landmark was L4.

This state enables IR sensor obstacle avoidance.


\item[finished]\hfill\\
Success!

\end{description}

See \texttt{execute_strategy} in \texttt{strategy.cc}, and \texttt{drive} and
\texttt{turn} in \texttt{robot.cc} for the implementation.


\newpage
\section{Discussion}

We have tested our code in multiple ways:

\begin{itemize}
\item Using just the webcam.  This is useful for figuring out whether our
particle filter worked.
\item Using the webcam and the robot with our full program and with a full
course.  This is obviously useful to see if the code works.
\item Using the webcam and the robot with a small test program.  This is useful
for figuring out whether the different components -- robot controls, camera, IR
sensors -- can work together.
\end{itemize}

We have made a single test program.  It requires the robot to be positioned in
the center of the four landmarks, and then turns the robot until it has seen
them all.  This test aims to test if the particle filter can correctly determine
in a real situation that the robot is in the center.  It works okay.

We have also run the full program quite a lot, fixing various small bugs.
However, we have had many problems with the Scorpion robots, so we did not
manage to run our code on the robot as much as we wanted.


\end{document}
